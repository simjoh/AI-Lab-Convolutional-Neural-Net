{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify ground type from sentinel-2 images\n",
    "In this laboration Sentinel-2 images containing 12 bands will be used in combination with another dataset from Naturv√•rdsverket. \n",
    "\n",
    "This laboration has many similarities with the previous. But we will also look more closely into the results and introduce concepts as True positive, False positive, True negative and false negative. And looka at the data to see what the model is good and bad at. \n",
    "\n",
    "We will have a closing discussion on how this laboration could improve in different ways by feature engineering, data selection for the trainingset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder #Used to convert or ground type class\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import normalize #\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file that contains all the data prepared for the lab\n",
    "try:\n",
    "    # Should work in local enviroment and on binder\n",
    "    path='./data2/ROI3_2_1_2018_smaland_virserum/MB_ROI3_2_1_2018_smaland_virserum_13Band.npz'\n",
    "    file=np.load(path)\n",
    "except:\n",
    "    # Should work at colab\n",
    "    import urllib.request, urllib.parse, urllib.error\n",
    "    import io\n",
    "    \n",
    "    binary = urllib.request.urlopen('https://raw.githubusercontent.com/simjoh/AI-Lab-Neural-Net-and-Python/master/data/ROI3_2_1_2018_smaland_virserum/MB_ROI3_2_1_2018_smaland_virserum_13Band.npz').read()\n",
    "    memfile = io.BytesIO(binary)\n",
    "    file=np.load(memfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMD\n",
    "To start with we need to look at the NMD data that we are going to use as the anwsere while we are training and validating or model. A good start is to plot it and se how i looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMD=file['orig_classed_img'] #\n",
    "all_bands=file['raw_img']\n",
    "plt.imshow(NMD)\n",
    "plt.title('NMD classes')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those numerica values doesn't make sense and the colormap does not help us to understand the landscape. But we can guess that dark blue is city and roads, blue is water and green to yellow is forests, crops and wetlands.\n",
    "\n",
    "In the cell below we try to vizualize it a little better then the default.\n",
    "\n",
    "It is not needed for this laboration to understand the `plot_ground` function it is a quite complicated function that uses some functions from `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ground(Y, encoder,title):\n",
    "    colormapping = { 41:[224, 224, 224],\n",
    "                     42:[254, 210, 125],\n",
    "                     62:[102, 153, 206],\n",
    "                     124:[57, 168, 0],\n",
    "                     127:[75, 232, 0],\n",
    "                     128:[191,  143, 191]}\n",
    "    cmap_colors = []\n",
    "\n",
    "    for i in encoder.classes_:\n",
    "        # The colors has to be converted into interval 0-1\n",
    "        c = np.array(colormapping.get(i))/255.\n",
    "        cmap_colors.append(c)\n",
    "    \n",
    "    cmap = colors.ListedColormap(cmap_colors)\n",
    "    ticks=encoder.classes_.copy()\n",
    "    bounds=[ticks[0]-0.5]\n",
    "    \n",
    "    i=1\n",
    "    while len(bounds)<len(ticks):\n",
    "        bounds.append(ticks[i]-0.5)\n",
    "        try:\n",
    "            #replace ticks to be in the middle of the interval\n",
    "            ticks[i]=(ticks[i]+ticks[i+1])/2.\n",
    "        except:\n",
    "            i+=0\n",
    "        i+=1\n",
    "    bounds.append(ticks[i-1]+0.5)\n",
    "\n",
    "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    fig2 = plt.figure(dpi=128,figsize=(8, 4))\n",
    "    plt.title(title)\n",
    "    ax1= plt.imshow(Y.reshape((500,500)), interpolation='nearest',\n",
    "                        cmap=cmap, norm=norm)\n",
    "    plt.axis('off')\n",
    "    cbar = fig2.colorbar(ax1, cmap=cmap, norm=norm, boundaries=bounds, ticks=ticks)\n",
    "    cbar.ax.set_yticklabels(['Urban', 'Field','Water','Coniferous','Deciduous','Rejuvenation'])\n",
    "    plt.show()\n",
    "    \n",
    "encoder = LabelEncoder()\n",
    "print('Shape of NMD data',NMD.shape)\n",
    "##\n",
    "NMD_df=pd.DataFrame()\n",
    "NMD_df['NMD'] = NMD.flatten()\n",
    "class_convertion = [('Urban',41,), ('Field',42),('Water',62),('Coniferous',124),('Deciduous',127),('Rejuvenation',128)]\n",
    "# Convert the NMD values to its label\n",
    "##\n",
    "encoder.fit(np.unique(NMD))\n",
    "plot_ground(NMD, encoder,'NMD')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script might have been a little complicated and might not give the most value for the lab. But it was necesary in order to make the data understandable. Specially in the long run when we evaluating the model. \n",
    "\n",
    "We can illustrate the amount of data by a bar plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMD_df=pd.DataFrame()\n",
    "NMD_df['NMD'] = NMD.flatten()\n",
    "class_convertion = [('Urban',41,), ('Field',42),('Water',62),('Coniferous',124),('Deciduous',127),('Rejuvenation',128)]\n",
    "# Convert the NMD values to its label\n",
    "for pair in class_convertion:\n",
    "    NMD_df.loc[NMD_df['NMD']==pair[1]]=pair[0]\n",
    "\n",
    "class_colors = [np.array([224, 224, 224])/255., \n",
    "         np.array([254, 210, 125])/255., \n",
    "         np.array([102, 153, 206])/255., \n",
    "         np.array([57, 168, 0])/255., \n",
    "         np.array([75, 232, 0])/255., \n",
    "         np.array([191,  143, 191])/255.]\n",
    "\n",
    "newPal   = dict(Urban = class_colors[0], \n",
    "                Field = class_colors[1], \n",
    "                Water = class_colors[2], \n",
    "                Coniferous = class_colors[3], \n",
    "                Deciduous = class_colors[4], \n",
    "                Rejuvenation = class_colors[5])\n",
    "\n",
    "ax = sns.countplot(x='NMD', data=NMD_df, hue='NMD',\n",
    "                 palette=newPal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the barplot we can see that it is a quite significant difference in the amount of datapoints in each class. Take a moment and think about the following questions.\n",
    "\n",
    "1. How can an uneven distribution between the classes in the trainingset impact the final model?\n",
    "2. What might be the possible solutions to attack such a problem?\n",
    "3. How can we know for certain that the classed image is correct?\n",
    "\n",
    "Those questions are very important to keep track on. For example if a company wants to predict need of maintanence with ML/AI but but there might only be one or two days out of 1000 that contains data of the machine running unevenly due to bad components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of the satellite images\n",
    "The satellite images from Sentinel-2 contains of 11-12 bands depending on year of photo and processing level of the photo. It is band 10 that is missing if there is only 11 bands. Each band is a registration of a specific wavelength. Note that different bands has different resolutions. \n",
    "\n",
    "If you want to read a little more about the sensors [Wikipedia](https://en.wikipedia.org/wiki/Sentinel-2) has a good overview. If you want more detailed information [ESA](https://earth.esa.int/web/sentinel/technical-guides/sentinel-2-msi) will  provide you with more then enough data.\n",
    "\n",
    "First we look at the data and try to get some idea of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Bands=['Band01','Band02','Band03','Band04','Band05','Band06','Band07','Band08','Band08A','Band9','Band11','Band12']\n",
    "\n",
    "nrows, ncols = 3, 4\n",
    "fig = plt.figure(figsize=(12,9))    \n",
    "i=0\n",
    "for band in Bands:\n",
    "    ax = fig.add_subplot(nrows, ncols, i+1)\n",
    "    ax.imshow(all_bands[i,:,:])\n",
    "    ax.title.set_text(band)\n",
    "    ax.axis('off')\n",
    "    i+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Band 2, 3 and 4 which are all in the visible spectrum have in common that they are very dark due to some very bright pixles in the SW corner. \n",
    "\n",
    "If you want and have time you can elaborate with line `ax.imshow(all_bands[i,:,:])` in the cell above and add  `, vmin=0, vmax=1000` before the trailing parathesis to improve dynamics of the image. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check what values that are in the matrix\n",
    "print(np.unique(NMD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to put our data into a dataframe it has to be restructured into a 2d matrix shaped dataset. Where each column is all the pixels from one band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_bands.reshape((12, np.shape(all_bands)[1]*np.shape(all_bands)[2])).T\n",
    "Y = NMD.reshape((np.shape(NMD)[0]*np.shape(NMD)[1],1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bands=['Band01','Band02','Band03','Band04','Band05','Band06','Band07','Band08','Band08A','Band09','Band11','Band12','NMD']\n",
    "\n",
    "data1 = np.c_[X,Y]\n",
    "df = pd.DataFrame(data1,columns=Bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the correlation matrix and name the correlation matrix `corr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation here\n",
    "corr=df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "cmap = sns.diverging_palette(2, 120, as_cmap=True)\n",
    "sns.heatmap(corr,  vmax=1,vmin=-1, center=0, cmap=cmap,# mask=mask,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the correlation matrix with 2 deciamls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there are some bands that are quite strongly correlated with each other. \n",
    "\n",
    "Delete bands that does not provide a lot of new data\n",
    "\n",
    "1. Create a list with all significant bands\n",
    "\n",
    "2. Calculate the correlation of those bands with the `corr()` method\n",
    "\n",
    "3. Make the heat map plot as above and look "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list with significant bands and name it sig_bands\n",
    "sig_bands=['Band01','Band02','Band08A','Band08','Band09','Band11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corr2=df[sig_bands].corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr2,  vmax=1,vmin=-1, center=0,cmap=cmap,# mask=mask,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .8}, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the correlation matrix with 2 decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for looking at the correlation matrix here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the pair plot with only the significant bands. Color them by the NMD value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training, test and validation data with only the selected variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data before splitting it train test and validation set.\n",
    "data = df#.sample(frac=1).reset_index(drop=True)\n",
    "data.iloc[:,:-1] = normalize(df.iloc[:,:-1].values,axis=0)\n",
    "\n",
    "train, validate, test = np.split(data.sample(frac=1), [int(.6*len(data)), int(.8*len(data))])\n",
    "\n",
    "# Split the data into train test and validation sets.\n",
    "train_x = train.iloc[:,:-1].values\n",
    "train_y = train.iloc[:,-1].values\n",
    "\n",
    "test_x = test.iloc[:,:-1].values\n",
    "test_y = test.iloc[:,-1].values\n",
    "\n",
    "validate_x = validate.iloc[:,:-1].values\n",
    "validate_y = validate.iloc[:,-1].values\n",
    "\n",
    "# Convert the label to a understandable format for tensorflow\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_y)\n",
    "\n",
    "train_label = le.transform(train_y)\n",
    "train_y = tf.keras.utils.to_categorical(train_label,num_classes=6)\n",
    "\n",
    "test_label = le.transform(test_y)\n",
    "test_y = tf.keras.utils.to_categorical(test_label,num_classes=6)\n",
    "\n",
    "validate_label = le.transform(validate_y)\n",
    "validate_y = tf.keras.utils.to_categorical(validate_label, num_classes=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the neural net with 2hidden layers and 5-10 nodes in each \n",
    "Create a neural net with dropout layers and a reasonable amount of neurons in each layer.\n",
    "\n",
    "Name the model into variable `model` and store the fitting into `history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10,input_dim=12,activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(8,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(6,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "history = model.fit(train_x,train_y,validation_data=(validate_x, validate_y),batch_size=512,epochs=50,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity lets predict the entire image that we trained or model on and se how it compares the the correct values.\n",
    "\n",
    "In a real case scenario we should evaluate the model with something unknown so it would be better to  evaluate it with another image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=data.copy()\n",
    "Y=X['NMD']#data.copy()\n",
    "X.pop('NMD')\n",
    "output = model.predict_classes(X, verbose=1,batch_size=512)\n",
    "print(output)\n",
    "temp = to_categorical(output)\n",
    "out_enc = le.inverse_transform(np.argmax(temp,axis=1))\n",
    "print(np.unique(out_enc))\n",
    "\n",
    "\n",
    "plot_ground(out_enc.reshape((500,500)),encoder,'Predicted')\n",
    "plot_ground(Y.values.reshape((500,500)),encoder,'NMD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall impression is that the method works pretty well. But the model predicts a very small amount of Rejuvenation forest. Those are either predicted as fields or forest. Also there is quite a lot of mix between Coniferous and Deciduous forests. \n",
    "\n",
    "The water areas are well predicted but there is small lake in northen middle part of the image that is missing from the predicted region. Most likely this lake is dried out when the satelite image is taken.\n",
    "\n",
    "Roads are not predicted very well but the city is well described. \n",
    "\n",
    "How come that the roads are not predicted so well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "In binary classification problems ROC curves can be used to evaluate the model. In multiple classification problems this could be possible. But would need one ROC curve for each class. Instead confusion matrices are used, where each row is the actual class and the column represents the prediction of the mode. \n",
    "\n",
    "Confusion matrix is a function in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(Y.values.flatten(), out_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(out_enc,columns=['pred'])\n",
    "for pair in class_convertion:\n",
    "    out_df.loc[out_df['pred']==pair[1]] = pair[0]\n",
    "\n",
    "pd.crosstab(NMD_df.NMD, out_df.pred, rownames=['Actual Ground Type'], colnames=['Predicted Ground Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the breast cancer laboration we trusted the dataset and expected it to be correct so we focused on how to improve the model. In this case we should not trust the NMD data to be accurate. \n",
    "\n",
    "Below we look at the NMD water class and compare it to the predictions of the validation data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_validation = model.predict_classes(validate_x, verbose=1,batch_size=512)\n",
    "water_pixels = validate.loc[(df['NMD']==62)]\n",
    "temp = to_categorical(predict_from_validation)\n",
    "predict_from_validation_class = le.inverse_transform(np.argmax(temp,axis=1))\n",
    "\n",
    "validate['predicted'] = predict_from_validation_class\n",
    "\n",
    "water_pixels_NMD = water_pixels.loc[validate['NMD']==62,['Band02','Band03','Band04','predicted']]\n",
    "\n",
    "w_correct = water_pixels.loc[water_pixels_NMD['predicted']==62].copy()\n",
    "\n",
    "w_false = water_pixels.loc[water_pixels_NMD['predicted']!=62].copy()\n",
    "\n",
    "\n",
    "filters = [w_correct, w_false]\n",
    "titles = ['Water correct', 'Water incorrect']\n",
    "i = 0\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,6))\n",
    "for vals in filters:\n",
    "    rgb = vals[['Band02','Band03','Band04']].values#*5000\n",
    "    s = np.shape(rgb)[0]\n",
    "    rows = np.ceil(np.sqrt(s))\n",
    "    cols = np.ceil(np.sqrt(s))\n",
    "\n",
    "    collected_pixels = np.zeros((int(rows*cols),3))\n",
    "    collected_pixels[:s,0] = rgb[:,2]\n",
    "    collected_pixels[:s,1] = rgb[:,1]\n",
    "    collected_pixels[:s,2] = rgb[:,0]\n",
    "    collected_pixels = (200.*collected_pixels) #scale  r g b with 200 to make it reasonable in colormap.\n",
    "    \n",
    "    ax[i].imshow(collected_pixels.reshape((int(rows),int(rows),3)))\n",
    "    ax[i].set_title(titles[i])\n",
    "    #ax[i].axis('off')\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the images above we can conclude that quite a lot of the area in the NMD data that is classed as water is really somehting else. Therefor it is not possible to use only NMD to validate or model when it is trained. \n",
    "\n",
    "In order to create a better model it might be neccesary to clean the training data from incorrect classed water. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_water = train.loc[(df['NMD']==62)]\n",
    "filtered_water = train.loc[(df['NMD']==62) &(df['Band03']<0.0009)]\n",
    "filters=[all_water,filtered_water]\n",
    "i=0\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,6))\n",
    "for vals in filters:\n",
    "    rgb = vals[['Band02','Band03','Band04']].values#*5000\n",
    "    s = np.shape(rgb)[0]\n",
    "    rows = np.ceil(np.sqrt(s))\n",
    "    cols = np.ceil(np.sqrt(s))\n",
    "\n",
    "    collected_pixels=np.zeros((int(rows*cols),3))\n",
    "    collected_pixels[:s,0]=rgb[:,2]\n",
    "    collected_pixels[:s,1]=rgb[:,1]\n",
    "    collected_pixels[:s,2]=rgb[:,0]\n",
    "    collected_pixels=(200.*collected_pixels)\n",
    "    \n",
    "    ax[i].imshow(collected_pixels.reshape((int(rows),int(rows),3)))\n",
    "    ax[i].axis('off')\n",
    "    i+=1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left image we can see all pixels that are classed as water in the NMD data. Some of those appears to be quite green for beeing water. \n",
    "\n",
    "In the right picture only pixels lower then a certain threshold is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_water = train.loc[(df['NMD']==62)]\n",
    "filtered_water = train.loc[(df['NMD']==62) & (df['Band03']<0.0009)]\n",
    "filters=[all_water,filtered_water]\n",
    "i=0\n",
    "fig,ax = plt.subplots(1,2,figsize=(12,6))\n",
    "for vals in filters:\n",
    "    rgb = vals[['Band02','Band03','Band04']].values\n",
    "    s = np.shape(rgb)[0]\n",
    "    rows = np.ceil(np.sqrt(s))\n",
    "    cols = np.ceil(np.sqrt(s))\n",
    "\n",
    "    collected_pixels=np.zeros((int(rows*cols),3))\n",
    "    collected_pixels[:s,0]=rgb[:,2]\n",
    "    collected_pixels[:s,1]=rgb[:,1]\n",
    "    collected_pixels[:s,2]=rgb[:,0]\n",
    "    collected_pixels=(200.*collected_pixels)#.astype(int)\n",
    "    \n",
    "    ax[i].imshow(collected_pixels.reshape((int(rows),int(rows),3)))\n",
    "    ax[i].axis('off')\n",
    "    i+=1\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(train[(train.NMD ==62 ) & (train.Band03>0.0009)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Look at the urban class and see how well this class is predicted and how the urban classification according to NMD data looks in the satellite image. \n",
    "2. Make a new bar plot and see how many pixels there is in each class.\n",
    "\n",
    "3. Train the model with the cleaned dataand see if there is any differnce.\n",
    "4. The model above is trained with all bands. Try to retrain it by removing bands that  might be duplications of other bands or that might be to low in resolution. Any differance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
