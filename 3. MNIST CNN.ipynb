{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification\n",
    "In this notebook we are solving an image classification problem using Convolutional neural net. \n",
    "\n",
    "The data we are going to work with is the MNIST dataset which contains of 70000 numbers evenly distributed between 0-9.\n",
    "\n",
    "The aim is to introduce the concept of convolutional layers and show its potential. The user should be familar with neural nets. This notebook is very similar to a notebook solving the problem with a traditional densly coupled neural net.\n",
    "\n",
    "Some efforts has been made to show how the convolutions and pooling layers are working together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "from scipy import ndimage\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# Run twice to close the annoying warning created by numpy and tensorflow\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          fig_size=10):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]*100\n",
    "    else:\n",
    "        cm = cm\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_size,fig_size))\n",
    "    im = ax.imshow(cm,norm=LogNorm(), cmap=cmap,\n",
    "                interpolation='nearest')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ylim=ax.get_ylim()\n",
    "    ax.set(\n",
    "        ylim=ylim,\n",
    "        xticks=np.arange(cm.shape[1]),\n",
    "        yticks=np.arange(cm.shape[0]),\n",
    "        # ... and label them with the respective list entries\n",
    "        xticklabels=classes, \n",
    "        yticklabels=classes,\n",
    "        title=title,\n",
    "        ylabel='True label',\n",
    "        xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "              rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.1f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def plot_errors(x_test, y_test, output,n_max=600):\n",
    "    \"\"\" Function the reporting in a script in order to\n",
    "    breake the script if it is estimated to take to long time\"\"\"\n",
    "    n_not_corr = np.sum(output != y_test )\n",
    "    n = int(np.ceil(np.sqrt(n_not_corr)))\n",
    "    j = 0\n",
    "    if n_not_corr > n_max:\n",
    "        print('more then '+str(n_max),n**2)\n",
    "        return\n",
    "    f, ax = plt.subplots(n, n, figsize=(25, 25))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i in range(np.shape(output)[0]):\n",
    "            if output[i]!=y_test[i]:\n",
    "                ax[j].set_title(str(y_test[i]) + ' as ' + str(output[i]))\n",
    "                ax[j].imshow(x_test[i,:,:,0], cmap='gray')\n",
    "                ax[j].axis('off')\n",
    "                j+=1\n",
    "    for x in ax.ravel():\n",
    "        x.axis(\"off\")\n",
    "    plt.subplots_adjust(bottom=-0.09, wspace=0.03)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "We also need to take a first look of the dataset by looking at the shape which gives us an idaea of the structure and size. We also show the dataset distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('Shape before reshape:',np.shape(x_train))\n",
    "x_train = x_train.reshape(-1,28, 28, 1)   #Reshape for CNN !!\n",
    "x_test = x_test.reshape(-1,28, 28, 1)     #The added dimension is to account for RRG images\n",
    "\n",
    "##\n",
    "print('Train data:',np.shape(x_train))\n",
    "print('Test data:', np.shape(x_test))\n",
    "\n",
    "y_count = np.bincount(y_train)\n",
    "ii = np.nonzero(y_count)[0]\n",
    "plt.bar(ii, y_count)\n",
    "plt.xticks(ii)\n",
    "plt.title('Distribution of numbers')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the dataset\n",
    "It is always good to get a good overview of the dataset. In this case when images are going to be classified a good may to explore the dataset is by looking at the images. If you don't trust the dataset you should go through all images and see if the classification is correct. \n",
    "\n",
    "Remember that there is a huge job done preparing and classifying all images. If you are going to do a classification task youself you first need to classify a huge amount of images.\n",
    "\n",
    "Note that some of the numbers are quite hard to classify even for the human eye. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(10, 10, figsize=(10, 10))\n",
    "ax = ax.flatten()\n",
    "for i in range(len(ax)):\n",
    "    ax[i].imshow(x_test[i, :, :, 0], cmap='gray')\n",
    "    ax[i].set_title(y_test[i])\n",
    "[axi.set_axis_off() for axi in ax.ravel()]\n",
    "plt.subplots_adjust(bottom=-0.09, wspace=0.03)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "Look at 100 different images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write some code that plots the last 100 images in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataset to have values between 0 and 1\n",
    "if np.max(x_train)>1:\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "else:\n",
    "    print('Already scaled once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "From the images above which numbers do you think are hardest and easiest for the model to classify? \n",
    "\n",
    "Which numbers do you think gets mixe up by the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the conv net\n",
    "Instead of Densly coupled layers that only cares about each pixel. We try to make a more general network that tries to see more general characteristics of the image. \n",
    "\n",
    "In this first example we are using convolutional layers to \"look\" at 3x3 pixels and then reduce the size of the image with MaxPooling layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Sequential Model and adding the layers\n",
    "input_shape = (28, 28, 1) #The shape of each image\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the model\n",
    "To get an idea of how many parameters we are trying to calculate and to get the model configuration we can use the summary method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of parameters\n",
    "In the first convolution layer there is only 280 parameters to calculate compared to the dens layer with over 600' parameters. In the convolution layer each filter is 3x3 hence 9 parameters is needed to describe the filter function. Additionally we have a bias for each filter then we have 28 filters in the convolutional layer. \n",
    "* (3 * 3 + 1) * 28 = 280\n",
    "\n",
    "\n",
    "Before the dense layer we have flattened the resulting image of the convolutional layer. \n",
    "* 13 * 13 * 28 = 4732\n",
    "\n",
    "Each neuron in the dense layer is connected to every neuron in the output of the previous layer resulting in:\n",
    "*  (4732 + 1) * 128 = 605824\n",
    "\n",
    "In the the classification layer there is 10 neurons, one for each class that are connected with the 128 output parameters. There is also one bias parameter for each node. \n",
    "* (28 + 1) * 10=290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training dataset\n",
    "history=model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "f,ax=plt.subplots(1,2,figsize=(12,4))\n",
    "ax[0].plot(history.history['acc'], label='train')\n",
    "ax[0].plot(history.history['val_acc'], label='test')\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "# summarize history for loss\n",
    "ax[1].plot(history.history['loss'], label='train')\n",
    "ax[1].plot(history.history['val_loss'], label='test')\n",
    "ax[1].set_title('Model loss / cost')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict_classes(x_test, verbose=1, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names= np.unique(y_test)\n",
    "plot_confusion_matrix(y_test, prediction, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix',fig_size=10)\n",
    "plt.show()\n",
    "print(classification_report(y_test, prediction, digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion about the confusion matrix\n",
    "Even though just two epochs was used to calculate classification model. It has come up with a rather good model. A very interesting observation in the confusion matrix above is that the model mixes numbers that are quite closesly looking for example 3 and 5, 7 and 2 and 4 and 9. It is quite easy to understand why and how those numbers can be confusing for the system.\n",
    "\n",
    "### Investigate the errors\n",
    "In the cell below all errors are plotted. In a lot of cases we can understand why the model predicted wrong. But in a lot of predictions we can expect the model to do better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, prediction, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a slightly better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Sequential Model and adding the layers\n",
    "input_shape = (28, 28, 1)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Flattening the 2D arrays for fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=256)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "f,ax=plt.subplots(1,2,figsize=(12,4))\n",
    "ax[0].plot(history.history['acc'], label='train')\n",
    "ax[0].plot(history.history['val_acc'], label='test')\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "# summarize history for loss\n",
    "ax[1].plot(history.history['loss'], label='train')\n",
    "ax[1].plot(history.history['val_loss'], label='test')\n",
    "ax[1].set_title('Model loss / cost')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the result\n",
    "One way to evaluate the result is throug the confusion matrix. \n",
    "### Exercise\n",
    "What is the most common error of our classification model?\n",
    "\n",
    "Compared to the artificial neural network. How does this setup work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict_classes(x_test, verbose=1, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, output, digits = 3))\n",
    "class_names= np.unique(y_test)#.astype(str)\n",
    "plot_confusion_matrix(y_test, prediction, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix',fig_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, output, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the filters/kernels\n",
    "In the presentation there where a few slides about the filters. In this model we have used 3x3 filters and max poolinglayers.\n",
    "\n",
    "### Exercise\n",
    "Look at the filters and try to determine what features are extracted/enhanced by that filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First plot \n",
    "f,ax=plt.subplots(1,11,figsize=(20,2))\n",
    "p=0\n",
    "for j in [1,3,5,7,2,11,13,38,17,4]:\n",
    "    im = x_train[j,:,:,0]\n",
    "    ax[p].imshow(im,cmap='gray')\n",
    "    p+=1\n",
    "for i in range(11):\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n",
    "\n",
    "for i in range(np.shape(model.layers[0].get_weights()[0])[-1]):\n",
    "    f,ax=plt.subplots(1,11,figsize=(20,2))\n",
    "    p=0\n",
    "    for j in [1,3,5,7,2,11,13,38,17,4]:\n",
    "        im = x_train[j,:,:,0]\n",
    "        k = model.layers[0].get_weights()[0][:,:,0,i]\n",
    "        im = ndimage.convolve(im, k)\n",
    "        im = measure.block_reduce(im, (2,2), np.max)\n",
    "        \n",
    "        ax[p].imshow(im,cmap='gray')\n",
    "        p+=1\n",
    "    ax[10].imshow(k,cmap='gray')\n",
    "    for i in range(11):\n",
    "        ax[i].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augumentation\n",
    "A simple trick to increase the dataset is to make small changes to the images for each training cycle. One of those image generators is ImageDataGenerator included in Keras. \n",
    "\n",
    "Below is a small simple code example of how to use ImageDataGenerator. Note that we are not allowing vertical or horizontal flip since numbers are orientation dependent. In opposite to faces where we could use horizontal flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# initialize the number of epochs and batch size\n",
    "EPOCHS = 2\n",
    "BS = 256\n",
    " \n",
    "# construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=5, \n",
    "                         zoom_range=0.05, \n",
    "                         width_shift_range=0.1, \n",
    "                         height_shift_range=0.1, \n",
    "                         shear_range=0.1, \n",
    "                         horizontal_flip=False, \n",
    "                         vertical_flip=False, \n",
    "                         fill_mode=\"nearest\")\n",
    "\n",
    "H = model.fit_generator(aug.flow(x_train, y_train, batch_size=BS),\n",
    "                        validation_data=(x_test , y_test), steps_per_epoch=len(x_train) // BS,\n",
    "                        epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict_classes(x_test, verbose=1, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, output, digits = 3))\n",
    "\n",
    "plot_confusion_matrix(y_test, output, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix',fig_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "f,ax=plt.subplots(1,2,figsize=(12,4))\n",
    "ax[0].plot(history.history['acc'], label='train')\n",
    "ax[0].plot(history.history['val_acc'], label='test')\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "# summarize history for loss\n",
    "ax[1].plot(history.history['loss'], label='train')\n",
    "ax[1].plot(history.history['val_loss'], label='test')\n",
    "ax[1].set_title('Model loss / cost')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "What kind of problems could this deep learning model solve in your organization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
