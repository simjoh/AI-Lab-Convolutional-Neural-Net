{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification\n",
    "In this notebook we are solving an image classification problem using Densly coupled neural net. \n",
    "\n",
    "The data we are going to work with is the MNIST dataset which contains 70000 handwritten numbers evenly distributed between 0-9. \n",
    "\n",
    "The aim is to show the concept of neural networks and give the user some basic knowledge of how to work with artificial neural networks in tensorflow.\n",
    "\n",
    "If you are working in colabs you can try to wotk on a GPU by click Edit -> Notebook Settings -> Hardware acceleration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load all libraries that are needed for the notebook\n",
    "# There are also some functions created to make life easier later on\n",
    "# If there are some warnings try to excecute the cell once again\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "# Some common filses used in laboration\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues,\n",
    "                          fig_size=10):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]*100\n",
    "    else:\n",
    "        cm = cm\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(fig_size,fig_size))\n",
    "    im = ax.imshow(cm,norm=LogNorm(), cmap=cmap,\n",
    "                interpolation='nearest')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ylim=ax.get_ylim()\n",
    "    ax.set(\n",
    "        ylim=ylim,\n",
    "        xticks=np.arange(cm.shape[1]),\n",
    "        yticks=np.arange(cm.shape[0]),\n",
    "        # ... and label them with the respective list entries\n",
    "        xticklabels=classes, \n",
    "        yticklabels=classes,\n",
    "        title=title,\n",
    "        ylabel='True label',\n",
    "        xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "              rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.1f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def plot_errors(x_test, y_test, output,n_max=600):\n",
    "    \"\"\" Function the reporting in a script in order to\n",
    "    breake the script if it is estimated to take to long time\"\"\"\n",
    "    n_not_corr = np.sum(output != y_test )\n",
    "    n = int(np.ceil(np.sqrt(n_not_corr)))\n",
    "    j = 0\n",
    "    if n_not_corr > n_max:\n",
    "        print('more then '+str(n_max),n**2)\n",
    "        return\n",
    "    f, ax = plt.subplots(n, n, figsize=(25, 25))\n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    for i in range(np.shape(output)[0]):\n",
    "            if output[i]!=y_test[i]:\n",
    "                ax[j].set_title(str(y_test[i]) + ' as ' + str(output[i]))\n",
    "                ax[j].imshow(x_test[i,:,:,0], cmap='gray')\n",
    "                ax[j].axis('off')\n",
    "                j+=1\n",
    "    for x in ax.ravel():\n",
    "        x.axis(\"off\")\n",
    "    plt.subplots_adjust(bottom=-0.09, wspace=0.03)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "Then take a first look of the dataset by looking at the shape which gives us an idaea of the structure and size. We also show the dataset distribution. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('Shape before reshape:',np.shape(x_train))\n",
    "x_train = x_train.reshape(-1,28, 28, 1)   #Reshape for CNN !!\n",
    "x_test = x_test.reshape(-1,28, 28, 1)     #The added dimension is to account for RRG images\n",
    "\n",
    "##\n",
    "print('Train data:',np.shape(x_train))\n",
    "print('Test data:', np.shape(x_test))\n",
    "\n",
    "y_count = np.bincount(y_train)\n",
    "ii = np.nonzero(y_count)[0]\n",
    "plt.bar(ii, y_count)\n",
    "plt.xticks(ii)\n",
    "plt.title('Distribution of numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As exercise look at the distribution of the test dataset\n",
    "# Write the code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the dataset\n",
    "It is always good to get a good overview of the dataset and explore it a little. In this case when images are going to be classified a good way is by looking at the images. If you don't trust the dataset you should go through the images and see if the classification is correct. \n",
    "\n",
    "Remember that there is a huge job done classifying all images. If you are going to do a classification task youself you first need to collect or create the images and then classify them. \n",
    "\n",
    "Note that some of the numbers are quite hard to classify even for the human eye. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(10, 10, figsize=(10, 10))\n",
    "ax=ax.flatten()\n",
    "for i in range(100):\n",
    "    ax[i].imshow(x_test[i, :, :, 0], cmap='gray')\n",
    "    ax[i].set_title(y_test[i])\n",
    "[axi.set_axis_off() for axi in ax.ravel()]\n",
    "plt.subplots_adjust(bottom=-0.09, wspace=0.03)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Look at the letters above and think a little about what numbers can be mixed up by a computer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the dataset to have values between 0 and 1\n",
    "if np.max(x_train)>1:\n",
    "    x_train = x_train / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "else:\n",
    "    print('Already scaled once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a first artificial neural network\n",
    "The network below is a simple network with a first layer that flattens the input image to the same amount of neurons as pixles in the image. Then there is a densly coupled layer with 64 neurons followed by a dropout layer. At the end there is a classification layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.3),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history=model.fit(x_train, y_train, validation_data=(x_test,y_test),epochs=20,batch_size=512)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "f,ax=plt.subplots(1,2,figsize=(12,4))\n",
    "ax[0].plot(history.history['acc'], label='train')\n",
    "ax[0].plot(history.history['val_acc'], label='test')\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "# summarize history for loss\n",
    "ax[1].plot(history.history['loss'], label='train')\n",
    "ax[1].plot(history.history['val_loss'], label='test')\n",
    "ax[1].set_title('Model loss / cost')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Run the code above without dropout layers. How does it change the model accuracy? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Neural network without dropout layers and run the fitting. Then plot ther result in this cell\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the result\n",
    "One way to evaluate the result is throug the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names= np.unique(y_test)\n",
    "predicted = model.predict_classes(x_test, verbose=1, batch_size=512)\n",
    "plot_confusion_matrix(y_test, predicted, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix',fig_size=10)\n",
    "plt.show()\n",
    "# plot_confusion_matrix\n",
    "print(classification_report(y_test, predicted, digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "By looking at the confusion matrix above what letters are most common to mix up? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(x_test, y_test, predicted, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a deeper net\n",
    "We can experiment with different configurations of the artificial neural network to get some improvements. The parameters that can be modified within each layer are:\n",
    "* Activation function (relu, sigmoid, etc.)\n",
    "* Number of neuron in each layer (32,64,128,256)\n",
    "* Fraction of Dropouts in each layer (0.1, 0.2, 0.3)\n",
    "\n",
    "Resulting in $2*5*3=30$ combinations\n",
    "\n",
    "Then we might want to evaluate the number of layers\n",
    "* Number of layers (1, 2, 3, 5)\n",
    "\n",
    "Note the quickly rise of configurations to run. Parameter optimization is an important question to have in mind!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Edit this network and run it as you like and compare the output with the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n",
    "  # Define some layers and check the difference\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy','acc'])\n",
    "\n",
    "history1=model1.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "f,ax=plt.subplots(1,2,figsize=(12,4))\n",
    "ax[0].plot(history1.history['acc'], label='train')\n",
    "ax[0].plot(history1.history['val_acc'], label='test')\n",
    "ax[0].set_title('Model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "\n",
    "# summarize history for loss\n",
    "ax[1].plot(history1.history['loss'], label='train')\n",
    "ax[1].plot(history1.history['val_loss'], label='test')\n",
    "ax[1].set_title('Model loss / cost')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[0].legend()\n",
    "plt.show()\n",
    "\n",
    "predicted1 = model1.predict_classes(x_test, verbose=1, batch_size=512)\n",
    "###\n",
    "plot_confusion_matrix(y_test, predicted1, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix',fig_size=10)\n",
    "plt.show()\n",
    "###\n",
    "conf_mat1 = confusion_matrix(y_test.flatten(), predicted1)\n",
    "\n",
    "print(classification_report(y_test, predicted1, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.plot_errors(x_test, y_test, predicted1,600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
