{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis of Breast Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this laboration we will try to diagnose breast cancer as Benign (not harmful) and Malignant (potentially harmfull). The dataset contians of quantified values of the X-ray image. The quantification of the image is part of the feature extraction which we will not focus on on this laboration. But it is a VERY important part in the ML/AI process. \n",
    "\n",
    "In this dataset each X-ray image has been quantified into 34 features describing the the image. The features describes the image in a manner that the Neural Net might understand, as numbers. \n",
    "\n",
    "Those features might be of more or less importance. It is for example likely that the id is not the best feature for classifying cancer type. To keep id is however a bad idea since it might have a strong correlation with id, for example if all cases diagnosed by id is hight. It is therefore always a good idea to remove irrelevant data from the dataset. Examples of features that might described the tumour might be, area, texture, peremiter, symmetry and smoothness of edge. \n",
    "\n",
    "It the following image examples of the two types of images are shown. For sure it is a difficult task to determine the type if you don't have the experience.\n",
    "\n",
    "![alt text](https://journals.plos.org/plosone/article/figure/image?size=large&id=info:doi/10.1371/journal.pone.0141357.g005 \"Example images of Benign and Malignant type cancers.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This diagnosis could also be done on the X-ray images alone without the feature extraction. Then image analysis and convolutional neural nets could be used. But probably the the results would not be as good with this limited dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we load all libraries needed for this notebook\n",
    " \n",
    "import pandas as pd # Load the Pandas libraries with alias 'pd'\n",
    "import tensorflow as tf # A library developed by google making GPU calculations simple\n",
    "import seaborn as sns # A nice library for plotting, alternative to matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file 'breast-cancer.csv' from the data folder\n",
    "# Open the csv to see the data\n",
    "try:\n",
    "    # If you run local or on binder\n",
    "    data = pd.read_csv(\"./data/breast-cancer.csv\") \n",
    "except:    \n",
    "    # If you run on colab\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/simjoh/AI-Lab-Neural-Net-and-Python/master/data/breast-cancer.csv\")\n",
    "\n",
    "# Preview the loaded data\n",
    "data.tail(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete uneccesary columns\n",
    "Detect columns that are unnecesary from the dataframe and delete the by using the pop method.\n",
    "\n",
    "If A is a dataframe with columns 'b' and 'c', column c can be deleted by  \n",
    "``` A.pop('b') ```\n",
    "\n",
    "Nothing will be deleted from the csv file so if you need to load the data again just run the cell above.\n",
    "\n",
    "Hint: Two columns should be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.pop('Unnamed: 32')\n",
    "data.pop('id')\n",
    "data.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the data\n",
    "There are lots of vizualising libraries in python. Many of them are based on matplotlib which shares many of the syntax with matlab. In this notebook seaborn is used to get an idea of the data and see if it might be possible to separate the two diagnosis from each other.\n",
    "\n",
    "1. Find the names of the columns by the  ``` columns ``` attribute of the dataframe and save it to a variable ``` cols ```. \n",
    "2. Delete diagnosis from the list of columns by the `drop` method.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the syntax here:\n",
    "cols=data.columns\n",
    "cols=cols.drop(['diagnosis'])\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make two pair plots one of the first five columns and another with column number 5 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(vars=cols[:5], data=data, hue='diagnosis', height=5)\n",
    "plt.show()\n",
    "sns.pairplot(vars=cols[5:10], data=data, hue='diagnosis', height=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data each time \n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the data into training and test data\n",
    "X = data.iloc[:,1:].values\n",
    "y = data.iloc[:,0].values\n",
    "X_normalized = normalize(X,axis=0)\n",
    "\n",
    "total_length = len(data)\n",
    "train_length = int(0.8*total_length)\n",
    "test_length = int(0.2*total_length)\n",
    "\n",
    "X_train = X_normalized[:train_length]\n",
    "X_test = X_normalized[train_length:]\n",
    "y_train = y[:train_length]\n",
    "y_test = y[train_length:]\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "train_label = le.transform(y_train)\n",
    "y_train = tf.keras.utils.to_categorical(train_label,num_classes=2)\n",
    "\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "le2.fit(y_test)\n",
    "test_label = le2.transform(y_test)\n",
    "y_test=tf.keras.utils.to_categorical(test_label,num_classes=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Sequential model whith tree hidden layers. 150,500 and 300 neurons in each layer. Use 'relu' as activation function for the hidden layers and 'softmax' for the output layer. \n",
    "\n",
    "Store the fitting parameters from the model into a variable called `history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(150,input_dim=30,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(500,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(300,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(2,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "history = model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=20,epochs=300,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate results\n",
    "By looking at the output from the model fit. We can se that the accuracy of training data keeps raising through the process of finding constants for each neuron in the net. However looking att the accuracy of the validation set there is some kind of maximum after quite few iteration. \n",
    "\n",
    "We can find a reasonable high amount epochs by plotting the history of the accuracy and loss. \n",
    "\n",
    "What is a reasonable amount of epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.ylim(0.95,1)\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss / cost')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better net?\n",
    "In the example above there are quite a lot of neurons in each layer of the net and the training proceduce goes on for quite a lot of iterations. This can cause overtraining which happens when the model is fitted to the training data in a way that it start to be unrealistic. Questions to reflect on in order to reduce overtraining:\n",
    "* When do we start to se overtraining.\n",
    "* How can overtraining be avoided\n",
    "* Do we see any diffrence in the loss and accuracy? \n",
    "* What is a reasonable amount if neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=tf.keras.Sequential()\n",
    "model2.add(tf.keras.layers.Dense(24,input_dim=30,activation='relu'))\n",
    "model2.add(tf.keras.layers.Dense(20,activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(10,activation='relu'))\n",
    "model2.add(tf.keras.layers.Dropout(0.2))\n",
    "model2.add(tf.keras.layers.Dense(2,activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "history2 = model2.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=20,epochs=300,verbose=1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history2.history['acc'])\n",
    "plt.plot(history2.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('model loss / cost')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3=tf.keras.Sequential()\n",
    "model3.add(tf.keras.layers.Dense(4,input_dim=30,activation='relu'))\n",
    "model3.add(tf.keras.layers.Dropout(0.2))\n",
    "model3.add(tf.keras.layers.Dense(2,activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "history3 = model3.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=20,epochs=300,verbose=1)\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history3.history['acc'])\n",
    "plt.plot(history3.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history3.history['loss'])\n",
    "plt.plot(history3.history['val_loss'])\n",
    "plt.title('model loss / cost')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "model4=tf.keras.Sequential()\n",
    "model4.add(tf.keras.layers.Dense(5,input_dim=30,activation='sigmoid'))\n",
    "model4.add(tf.keras.layers.Dense(2,activation='softmax'))\n",
    "model4.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])\n",
    "history4 = model4.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=20,epochs=500,verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history4.history['acc'])\n",
    "plt.plot(history4.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history4.history['loss'])\n",
    "plt.plot(history4.history['val_loss'])\n",
    "plt.title('model loss / cost')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is good?\n",
    "Depending on application the concept of a \"good\" model might be very different. It might be very important to catch all Positives and it might be alright to have a few False Positives. \n",
    "\n",
    "One way to evaluate a binary class model is throug receiver operating characteristic curves (ROC)  and it's quantity area under curve (AUC). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "models=[(model, 'model'), (model2, 'model2'), (model3, 'model3'), (model4, 'model4')]\n",
    "\n",
    "for mod in models:\n",
    "    \n",
    "    probs = mod[0].predict_proba(X_train)\n",
    "    \n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(np.argmax(y_train,axis=1), preds)\n",
    "    \n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    # method I: plt\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, label = '%s (AUC = %0.3f)' % (mod[1],roc_auc))\n",
    "    plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "for mod in models:\n",
    "    probs = mod[0].predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    fpr, tpr, threshold = metrics.roc_curve(np.argmax(y_test,axis=1), preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    # method I: plt\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, label = '%s (AUC = %0.3f)' % (mod[1],roc_auc))\n",
    "    plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
